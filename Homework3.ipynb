{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kMy8khN8tvG"
   },
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAY0edmR8tvI"
   },
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "downloading de-en.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "de-en.tgz: 100%|██████████| 24.2M/24.2M [00:16<00:00, 1.43MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
      ".data/iwslt/de-en/train.tags.de-en.en\n",
      ".data/iwslt/de-en/train.tags.de-en.de\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader('cpu')\n",
    "train_iter, val_iter, DE, EN = loader.get_iters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEjFvvFg8tvw"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/harvard-cs287-s19-hw3/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "EXcYOO1H8tvx",
    "outputId": "52671600-41df-4d32-aa19-e933cb0fbd02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100 50587  100 50587    0     0   250k      0 --:--:-- --:--:-- --:--:--  250k\n",
      "Als ich in meinen 20ern war , hatte ich meine erste Psychotherapie-Patientin .\n",
      "Ich war Doktorandin und studierte Klinische Psychologie in Berkeley .\n",
      "Sie war eine 26-jährige Frau namens Alex .\n",
      "Und als ich das hörte , war ich erleichtert .\n",
      "Meine Kommilitonin bekam nämlich einen Brandstifter als ersten Patienten .\n",
      "Und ich bekam eine Frau in den 20ern , die über Jungs reden wollte .\n",
      "Das kriege ich hin , dachte ich mir .\n",
      "Aber ich habe es nicht hingekriegt .\n",
      "Arbeit kam später , Heiraten kam später , Kinder kamen später , selbst der Tod kam später .\n",
      "Leute in den 20ern wie Alex und ich hatten nichts als Zeit .\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW3/source_test.txt\n",
    "!head source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbMPYOCW8tv1"
   },
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVSLUg5e8tv2"
   },
   "source": [
    "```\n",
    "Id,Predicted\n",
    "0,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "1,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "2,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "3,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8yp7q_D8tv2"
   },
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lo0EGJRn8tv4"
   },
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-3djaHv8tv6"
   },
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.\n",
    "\n",
    "The test data associated with `source_test.txt` can be found [here](https://gist.githubusercontent.com/justinchiu/c4340777fa86facd820c59ff4d84c078/raw/e6ec7daba76446bc1000813680f4722060e51900/gistfile1.txt). Compute the BLEU score of your conditional de-en model with the `multi-bleu.perl` script found [here](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl). Please submit your BLEU scores on test with your final writeup using the template provided in the repository:  https://github.com/harvard-ml-courses/nlp-template. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
