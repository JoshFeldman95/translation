{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kMy8khN8tvG"
   },
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAY0edmR8tvI"
   },
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "building vocab...\n",
      "initializing iterators...\n"
     ]
    }
   ],
   "source": [
    "from load_data import DataLoader\n",
    "#from models import LSTMTranslator, AttentionTranslator\n",
    "\n",
    "loader = DataLoader('cpu')\n",
    "train_iter, val_iter, DE, EN = loader.get_iters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(ntorch.nn.Module):\n",
    "    def __init__(self, DE, emb_dim, hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.embedding = ntorch.nn.Embedding(len(DE.vocab), emb_dim).spec('srcSeqlen','embedding')\n",
    "        self.rnn = ntorch.nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, bidirectional = self.attention).spec(\"embedding\", \"srcSeqlen\", \"lstm\")\n",
    "        self.dropout = ntorch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if not self.attention:\n",
    "            # reverse input\n",
    "            src = src[{'srcSeqlen':slice(-1,0)}]\n",
    "        \n",
    "        # run net\n",
    "        x = self.embedding(src)\n",
    "        x = self.dropout(x)\n",
    "        outputs, hidden = self.rnn(x)\n",
    "        if self.attention:\n",
    "            return {'src':outputs}\n",
    "        else:\n",
    "            return hidden\n",
    "\n",
    "class LSTMDecoder(ntorch.nn.Module):\n",
    "    def __init__(self, EN, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = ntorch.nn.Embedding(len(EN.vocab), emb_dim).spec('trgSeqlen','embedding')\n",
    "        self.rnn = ntorch.nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout).spec(\"embedding\", \"trgSeqlen\", \"lstm\")\n",
    "        self.out = ntorch.nn.Linear(hid_dim, len(EN.vocab)).spec(\"lstm\", \"logit\")\n",
    "        self.dropout = ntorch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, hidden):\n",
    "        x = self.embedding(trg)\n",
    "        x = self.dropout(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.out(x)\n",
    "        return x, hidden\n",
    "\n",
    "class AttentionDecoder(ntorch.nn.Module):\n",
    "    def __init__(self, EN, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = ntorch.nn.Embedding(len(EN.vocab), emb_dim).spec('trgSeqlen','embedding')\n",
    "        self.rnn = ntorch.nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout).spec(\"embedding\", \"trgSeqlen\", \"lstm\")\n",
    "        self.out = ntorch.nn.Linear(hid_dim*2, len(EN.vocab)).spec(\"lstm\", \"logit\")\n",
    "        self.dropout = ntorch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, hidden):\n",
    "        # get hidden state\n",
    "        src = hidden['src']\n",
    "        rnn_state = hidden['rnn_state'] if 'rnn_state' in hidden else None\n",
    "        \n",
    "        #run net\n",
    "        x = self.embedding(trg)\n",
    "        x = self.dropout(x)\n",
    "        if rnn_state is not None:\n",
    "            x, rnn_state = self.rnn(x, rnn_state)\n",
    "        else:\n",
    "            x, rnn_state = self.rnn(x)\n",
    "        context = x.dot('lstm', src).softmax('srcSeqlen').dot('srcSeqlen',src)\n",
    "        x = self.out(ntorch.cat([context, x], dim = 'lstm'))\n",
    "        \n",
    "        # create new hidden state\n",
    "        hidden = {'src': src, 'rnn_state':rnn_state}\n",
    "        return x, hidden\n",
    "\n",
    "class Translator(ntorch.nn.Module):\n",
    "    def __init__(self, teacher_forcing, device):\n",
    "        super().__init__()\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        #get src encoding\n",
    "        hidden = self.encoder(src)\n",
    "        \n",
    "        # initialize outputs\n",
    "        output_tokens = [trg[{'trgSeqlen':slice(0,1)}]]\n",
    "        output_distributions = []\n",
    "        \n",
    "        # make predictions\n",
    "        for t in range(trg.shape['trgSeqlen']-1):\n",
    "            #predict next word\n",
    "            if random.random() < self.teacher_forcing:\n",
    "                inp = trg[{'trgSeqlen':slice(t,t+1)}]\n",
    "                out, hidden = self.decoder(inp, hidden)\n",
    "            else:\n",
    "                out, hidden = self.decoder(output_tokens[t], hidden)\n",
    "            \n",
    "            #store output\n",
    "            output_distributions.append(out)\n",
    "            _, top1 = out.max(\"logit\")\n",
    "            output_tokens.append(top1)\n",
    "        \n",
    "        #format predictions\n",
    "        return ntorch.cat(output_distributions, dim = 'trgSeqlen')\n",
    "    \n",
    "    def fit(self, train_iter, val_iter=[], lr=1e-2, verbose=True,\n",
    "        batch_size=128, epochs=10, interval=1, early_stopping=False):\n",
    "        self.to(self.device)\n",
    "        lr = torch.tensor(lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        train_iter.batch_size = batch_size\n",
    "\n",
    "        for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            self.train()\n",
    "            for i, data in enumerate(train_iter, 0):\n",
    "                src, trg = data.src, data.trg\n",
    "                optimizer.zero_grad()\n",
    "                out = self(src, trg)\n",
    "                loss = criterion(\n",
    "                    out.transpose(\"batch\", \"logit\", \"trgSeqlen\").values,\n",
    "                    trg[{\"trgSeqlen\":slice(1,trg.shape[\"trgSeqlen\"])}].transpose(\"batch\", \"trgSeqlen\").values,\n",
    "                )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # print statistics\n",
    "                if i % interval == interval - 1:  # print every 2000 mini-batches\n",
    "                    if verbose:\n",
    "                        print(f\"[epoch: {epoch + 1}, batch: {i + 1}] loss: {running_loss / interval}\")\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            running_loss = 0.0\n",
    "            val_count = 0.0\n",
    "            self.eval()\n",
    "            for i, data in enumerate(val_iter):\n",
    "                src, trg = data.src, data.trg\n",
    "                out = self(src, trg, teacher_forcing_ratio = 0)\n",
    "                loss = criterion(\n",
    "                    out.transpose(\"batch\", \"logit\", \"trgSeqlen\").values, \n",
    "                    trg[{\"trgSeqlen\":slice(1,trg.shape[\"trgSeqlen\"])}].transpose(\"batch\", \"trgSeqlen\").values\n",
    "                )\n",
    "                running_loss += loss.item()\n",
    "                val_count += 1\n",
    "            prev_loss = self.val_loss\n",
    "            self.val_loss = running_loss / val_count\n",
    "            if verbose:\n",
    "                print(f'Val loss: {self.val_loss}, PPL: {np.exp(self.val_loss)}')\n",
    "            if self.val_loss > prev_loss and early_stopping:\n",
    "                break\n",
    "            lr *= .8\n",
    "\n",
    "class LSTMTranslator(Translator):\n",
    "    def __init__(self, DE, EN, src_emb_dim, trg_emb_dim, hid_dim, n_layers = 4, dropout = 0.5, teacher_forcing = 0.75, device = 'cpu'):\n",
    "        super().__init__(teacher_forcing, device)\n",
    "        self.encoder = LSTMEncoder(DE, src_emb_dim, hid_dim, n_layers, dropout, False)\n",
    "        self.decoder = LSTMDecoder(EN, trg_emb_dim, hid_dim, n_layers, dropout)\n",
    "\n",
    "class AttentionTranslator(Translator):\n",
    "    def __init__(self, DE, EN, src_emb_dim, trg_emb_dim, hid_dim, n_layers = 4, dropout = 0.5, teacher_forcing = 0.75, device = 'cpu'):\n",
    "        super().__init__(teacher_forcing, device)\n",
    "        self.encoder = LSTMEncoder(DE, src_emb_dim, hid_dim, n_layers, dropout, True)\n",
    "        self.decoder = AttentionDecoder(EN, trg_emb_dim, hid_dim*2, n_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LSTMEncoder(DE, 300, 200, 4, 0.5, False)\n",
    "hidden = enc(batch.src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = LSTMDecoder(EN, 400, 200, 4, 0.5)\n",
    "out, hidden = dec(batch.trg[{'trgSeqlen':slice(0,1)}], hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LSTMEncoder(DE, 300, 200, 4, 0.5, True)\n",
    "hidden = enc(batch.src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = AttentionDecoder(EN, 400, 400, 4, 0.5)\n",
    "out = dec(batch.trg[{'trgSeqlen':slice(0,1)}], hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batch: 1] loss: 9.359054565429688\n",
      "[epoch: 1, batch: 2] loss: 8.621251106262207\n",
      "[epoch: 1, batch: 3] loss: 6.06719446182251\n",
      "[epoch: 1, batch: 4] loss: 5.063589572906494\n",
      "[epoch: 1, batch: 5] loss: 4.440306663513184\n",
      "[epoch: 1, batch: 6] loss: 4.90449333190918\n",
      "[epoch: 1, batch: 7] loss: 4.6908135414123535\n",
      "[epoch: 1, batch: 8] loss: 5.064265251159668\n",
      "[epoch: 1, batch: 9] loss: 4.687571048736572\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTranslator(DE, EN, 300, 300, 200)\n",
    "model.fit(train_iter,val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionTranslator(DE, EN, 300, 300, 200)\n",
    "model.fit(train_iter,val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
